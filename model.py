import torch
import torch.nn as nn


class PackingModel(nn.Module):
    """
    The learning module that generates the action probability distribution and predicted value from the current state.
    The action probabilities for the choice of object index and position are generated by scaling
    the sample_number of sigmoid outputs to the length of the list of objects and the size of each dimension of the bin.
    Each of those scaled values are binned and those bin counts are softmaxed to get the distribution.
    That means there is no set of learnable parameters dependent on bin shape and number of items.
    """

    def __init__(self, num_samples, embedding_dim, num_heads, num_layers):
        """
        Need to process the bin and all objects.
        3d conv all until GAP to embed each into a vector of size embedding_dim
        stack all the vectors of the bin and the objects then compute the attention between them
        :param num_samples: the number of samples generated for action probabilities
        :param embedding_dim: the size of the embedding space
        :param num_heads: the number of heads to use in the transformer
        :param num_layers: the number of encoder layers to use in the transformer
        """
        super().__init__()

        # store this for later
        self.num_samples = num_samples

        # The convolutional layers. Potentially replace with blocks like resnet
        self.conv1 = nn.Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=1)
        self.conv2 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=1)
        self.conv3 = nn.Conv3d(32, embedding_dim, kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=1)

        # The transformer encoder.
        encode_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward=512,
                                                  activation=nn.functional.silu, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encode_layer, num_layers)

        # potentially todo have layers before seqlen elimination

        # Layers for predicting object index
        # potentially todo use conv1d instead of linear to compress dim
        self.index1 = nn.Linear(embedding_dim, 1)
        self.index2 = nn.Linear(embedding_dim, num_samples)

        # Layers for predicting the position
        self.pos1 = nn.Linear(embedding_dim, 1)
        self.pos_x = nn.Linear(embedding_dim, num_samples)
        self.pos_y = nn.Linear(embedding_dim, num_samples)
        self.pos_z = nn.Linear(embedding_dim, num_samples)

        # Layers for predicting the rotation
        self.rot1 = nn.Linear(embedding_dim, 1)
        self.rot2 = nn.Linear(embedding_dim, 12)  # 12 because 3 axis and 4 rotation options

        # Layers for predicting the value of the state
        self.val1 = nn.Linear(embedding_dim, 1)
        self.val2 = nn.Linear(embedding_dim, 1)

        # group them for easy access later
        self.encoding_layers = [self.conv1, self.conv2, self.conv3, self.transformer_encoder]
        self.policy_layers = [self.index1, self.index2,
                              self.pos1, self.pos_x, self.pos_y, self.pos_z,
                              self.rot1, self.rot2]
        self.value_layers = [self.val1, self.val2]

    def encode(self, x):
        """
        distill the state representation
        :param x: a list of rank 5 tensors (b,c,l,w,h)
        :return: a tensor with shape (b, embedding_dim, embedding dim)
        """
        x = x.copy()
        # Convolve each tensor. I want to transform the info into a dense representation.
        for i in range(len(x)):
            x[i] = self.conv1(x[i])
            x[i] = nn.functional.silu(x[i])
            x[i] = self.conv2(x[i])
            x[i] = nn.functional.silu(x[i])
            x[i] = self.conv3(x[i])
            x[i] = nn.functional.silu(x[i])

        # Global average pool each tensor.
        x = [torch.mean(x[i], dim=(2, 3, 4)) for i in range(len(x))]

        # Concatenate the tensors.
        # Potentially concat a mask for bin vs object
        x = torch.stack(x, dim=1)  # (b, l, c)

        # Encode the tensor. No positional encoding since these aren't sequential points.
        x = self.transformer_encoder(x)  # expects (batch, seq, feat)
        # potentially layers hhere see .init comment
        x = torch.matmul(x.permute(0, 2, 1), x)  # should get (b, embed, embed) to remove dependence on seq

        return x

    def get_policy(self, x):
        """
        get the action policy
        :param x: the embedded state representation
        :return: the samples for actions of object index, position, and rotation
        """
        # get index distribution
        index = self.index1(x)  # returns (batch, embed, 1)
        index = nn.functional.silu(index.squeeze(dim=2))  # returns (batch, embed)
        index = self.index2(index)  # returns (batch, num_samples)
        index = nn.functional.sigmoid(index)

        # get position distribution
        pos = self.pos1(x)
        pos = nn.functional.silu(pos.squeeze(dim=2))
        pos_x = nn.functional.sigmoid(self.pos_x(pos))  # (b, num_samples)
        pos_y = nn.functional.sigmoid(self.pos_y(pos))
        pos_z = nn.functional.sigmoid(self.pos_z(pos))

        # get rotation distribution
        rot = self.rot1(x)
        rot = self.rot2(rot.squeeze(dim=2))
        rot = nn.functional.sigmoid(rot.reshape(rot.shape[0], 3, 4))  # (b, 3, 4)

        return index, (pos_x, pos_y, pos_z), rot

    def get_value(self, x):
        """
        get the predicted state value
        :param x: the embedded state representation
        :return: the predicted value of the state
        """
        val = self.val1(x)  # returns (batch, embed, 1)
        val = nn.functional.silu(val.squeeze(dim=2))  # returns (batch, embed)
        val = self.val2(val)  # returns (batch, 1)

        return val

    def forward(self, x):
        """
        determine which action to take based on the state x
        :param x: a list of rank 5 tensors (b,c,l,w,h)
        :return: a probability distribution of which item at which position with which rotation
        """

        x = self.encode(x)

        policy = self.get_policy(x)

        value = self.get_value(x)

        return policy, value



