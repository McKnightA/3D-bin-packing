import torch
import numpy as np


def discount_rewards(rewards, gamma=0.99):
    """
    Return discounted rewards based on the given rewards and gamma param.
    :param rewards: rewards from the environment
    :param gamma: how much to discount rewards in the future
    :return:
    """
    new_rewards = [float(rewards[-1])]
    for i in reversed(range(len(rewards) - 1)):
        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])
    return np.array(new_rewards[::-1])


def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):
    """
    Return the General Advantage Estimates from the given rewards and values.
    Paper: https://arxiv.org/pdf/1506.02438.pdf
    :param rewards: the rewards returned by the environment
    :param values: the predicted value of the state determined by the model
    :param gamma:
    :param decay:
    :return:
    """
    next_values = np.concatenate([values[1:], [0]])
    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]

    gaes = [deltas[-1]]
    for i in reversed(range(len(deltas) - 1)):
        gaes.append(deltas[i] + decay * gamma * gaes[-1])

    return np.array(gaes[::-1])


def get_action_distributions(samples, num_objects, bin_dims, remaining_objects, device='cpu'):
    """
    determine the distributions for actions then sample an action and determine that action's log probability
    :param samples: the samples generated by the model
    :param num_objects: the number of objects in the environment
    :param bin_dims: the dimensions of the bin (x, y, z)
    :param remaining_objects: the objects remaining to be placed in the bin
    :param device: the device to operate on
    :return: the action distributions
    """
    index_samples, pos_samples, rot_logits = samples

    index_logits = torch.zeros((index_samples.shape[0], num_objects), device=device)
    pos_x_logits = torch.zeros((len(pos_samples[0]), bin_dims[0]), device=device)
    pos_y_logits = torch.zeros((len(pos_samples[1]), bin_dims[1]), device=device)
    pos_z_logits = torch.zeros((len(pos_samples[2]), bin_dims[2]), device=device)

    # transform sigmoid samples into logits
    # the original samples are being altered, so potentially change it to work on a copy
    for i in range(index_samples.shape[1]):
        ind = index_samples[:, i]
        ind *= num_objects
        ind = ind.int()
        # Sometimes it breaks from an index error whenever sigmoid returns 1 because of precision limits
        ind[torch.where(ind == num_objects)] = num_objects - 1
        index_logits[np.arange(index_logits.shape[0]), ind] += 1

        pos_x = pos_samples[0][:, i]
        pos_x *= bin_dims[0]
        pos_x = pos_x.int()
        pos_x[torch.where(pos_x == bin_dims[0])] = bin_dims[0] - 1
        pos_x_logits[np.arange(pos_x_logits.shape[0]), pos_x] += 1

        pos_y = pos_samples[1][:, i]
        pos_y *= bin_dims[1]
        pos_y = pos_y.int()
        pos_y[torch.where(pos_y == bin_dims[1])] = bin_dims[1] - 1
        pos_y_logits[np.arange(pos_y_logits.shape[0]), pos_y] += 1

        pos_z = pos_samples[2][:, i]
        pos_z *= bin_dims[2]
        pos_z = pos_z.int()
        pos_z[torch.where(pos_z == bin_dims[2])] = bin_dims[2] - 1
        pos_z_logits[np.arange(pos_z_logits.shape[0]), pos_z] += 1

    # make logits -1 for indices that have already been placed so that they have a frequency unreachable by counting up
    for i in range(num_objects):
        if remaining_objects[i] is None:
            index_logits[:, i] = -100

    # determine the actions and the log probabilities of those actions based on the
    ind_distribution = torch.distributions.categorical.Categorical(logits=index_logits)

    pos_x_distribution = torch.distributions.categorical.Categorical(logits=pos_x_logits)
    pos_y_distribution = torch.distributions.categorical.Categorical(logits=pos_y_logits)
    pos_z_distribution = torch.distributions.categorical.Categorical(logits=pos_z_logits)

    rot_x_distribution = torch.distributions.categorical.Categorical(logits=rot_logits[:, 0])
    rot_y_distribution = torch.distributions.categorical.Categorical(logits=rot_logits[:, 1])
    rot_z_distribution = torch.distributions.categorical.Categorical(logits=rot_logits[:, 2])

    distributions = [ind_distribution, pos_x_distribution, pos_y_distribution, pos_z_distribution,
                     rot_x_distribution, rot_y_distribution, rot_z_distribution]

    return distributions


def rollout(model, env, max_steps=1000, objects=None, device='cpu'):
    """
    Performs a single rollout.
    :param model: the model to use as the action policy and value estimator
    :param env: the environment to act in
    :param max_steps: the maximum number of steps to take in the environment
    :param objects: the new objects to pack into the environment
    :param device: the device to run the rollout on
    :return: training data and the cumulative reward
    """
    # Create data storage
    train_data = [[], [], [], [], []]  # obs, act, reward, values, act_log_probs
    obs = list(env.reset(objects))
    obs[0] = [torch.Tensor(obs[0]).unsqueeze(0).unsqueeze(0).to(device)]
    obs[1] = [torch.Tensor(obj).unsqueeze(0).unsqueeze(0).to(device) for obj in obs[1]]
    obs = obs[0] + obs[1]

    ep_reward = 0
    for _ in range(max_steps):
        logits, val = model(obs)

        val = val.item()

        distributions = get_action_distributions(logits, len(env.objects), env.bin.shape, env.remaining_objects, device)

        # determine the actions and their log probabilities based on their distributions
        acts = []
        act_log_probs = []
        for distribution in distributions:
            acts.append(distribution.sample())
            act_log_probs.append(distribution.log_prob(acts[-1]).item())

        # the environment expects a certain data format
        act = (acts[0].item(),
               (acts[1].item(), acts[2].item(), acts[3].item()),
               (acts[4].item(), acts[5].item(), acts[6].item()))

        next_obs, reward, done, _ = env.step(act)

        for i, item in enumerate((obs, acts, reward, val, act_log_probs)):
            if i == 0:
                item = [x.cpu() for x in item]
            elif i == 1:
                item = [x.item() for x in item]
            train_data[i].append(item)

        obs = list(next_obs)
        obs[0] = [torch.Tensor(obs[0]).unsqueeze(0).unsqueeze(0).to(device).float()]
        obs[1] = [torch.zeros_like(torch.tensor(env.objects[i])).unsqueeze(0).unsqueeze(0).to(device).float()
                  if obs[1][i] is None
                  else torch.Tensor(obs[1][i]).unsqueeze(0).unsqueeze(0).to(device).float()
                  for i in range(len(obs[1]))]
        obs = obs[0] + obs[1]
        ep_reward += reward
        if done:
            break

    train_data = [np.asarray(x) if i > 0 else x for i, x in enumerate(train_data)]

    # Replace the values with the advantage estimates
    train_data[3] = calculate_gaes(train_data[2], train_data[3])

    return train_data, ep_reward


def generate_cubes(quantity, size):
    blocks = []
    for _ in range(quantity):
        blocks.append(np.ones((size, size, size)))
    return blocks


def generate_random_cuboids(quantity, shape):
    """
    generate a certain number of random cuboids
    :param quantity: how many cuboids are desired
    :param shape: ([possible x dimensions], [possible y dimensions], [possible z dimensions])
    :return: a list of np arrays
    """
    blocks = []
    max_dim = max(max(shape))
    for _ in range(quantity):
        block = np.zeros((max_dim, max_dim, max_dim))
        block[:np.random.choice(shape[0]),
              :np.random.choice(shape[1]),
              :np.random.choice(shape[2])] = 1

        blocks.append(block)

    return blocks


def mimic_quanqing_boxes(quantity, length, width):
    """
    sample l from[length/10, length/2], w from [width/10, width/2],
    h from [min(length, width)/10, max(length, width)/2]
    :param quantity: the number of desired cuboids
    :param length: the length of the environment
    :param width: the width of the environment
    :return: a list of np arrays
    """
    boxes = []
    max_dim = int(max(length, width)/2)
    for _ in range(quantity):
        block = np.zeros((max_dim, max_dim, max_dim))
        block[:np.random.randint(int(length/10), int(length/2)),
              :np.random.randint(int(width/10), int(width/2)),
              :np.random.randint(int(min(length, width)/10), int(max(length, width)/2))] = 1

        boxes.append(block)

    return boxes


def eval_quanqing_boxes(model, env, quantity, device='cpu'):
    """
    1. create some fixed sets of boxes
    2. rollout each set of boxes
    3. average the packing density of each of the fixed sets
    :param model: the model that's being evaluated
    :param env: the environment that the boxes are being packed into
    :param quantity: the number of boxes to pack
    :param device: where will the rollouts happen
    :return: the score of the model
    """

    length = env.bin.shape[0]
    width = env.bin.shape[1]
    scores = []
    remaining_blocks = []
    rng = np.random.default_rng(seed=46290)

    for i in range(30):  # get a few to calc the average

        # step 1
        boxes = []
        max_dim = int(max(length, width) / 2)
        for _ in range(quantity):
            block = np.zeros((max_dim, max_dim, max_dim))
            block[:rng.integers(int(length / 10), int(length / 2)),
                  :rng.integers(int(width / 10), int(width / 2)),
                  :rng.integers(int(min(length, width) / 10), int(max(length, width) / 2))] = 1

            boxes.append(block)

        # step 2
        a, b = rollout(model, env, objects=boxes, device=device)
        scores.append(env.calc_reward())

        unplaced_obj_count = 0
        for obj in env.remaining_objects:
            if obj is not None:
                unplaced_obj_count += 1
        remaining_blocks.append(unplaced_obj_count)

    # step 3
    return np.average(scores), remaining_blocks

